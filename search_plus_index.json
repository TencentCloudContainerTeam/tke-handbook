{"./":{"url":"./","title":"序","keywords":"","body":"TKE 指南 本书内容包含TKE的使用问题、技巧以及k8s本身的使用、踩坑经验、最佳实践、问题定位技巧等，形成一个系统化的参考指南以方便查阅，欢迎大家一起来补充完善。 LICENSE MIT © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"tke-skill/using-nat-gateway-visit-internet.html":{"url":"tke-skill/using-nat-gateway-visit-internet.html","title":"利用 NAT 网关实现无公网 IP 机器访问外网","keywords":"","body":"利用 NAT 网关实现无公网 IP 机器访问外网 先确定需要访问外网的无公网 IP 机器所属 VPC 及其子网 (购买机器的时候会让你选择，在机器实例详情页面也能看到) 进入 NAT网关 页面新建 NAT 网关，所属网络选择无公网机器所属 VPC 名称 在 路由表 页面找到机器所属路由器，在路由器详情页面的关联子网标签页可以看到该路由器所管辖的子网(每个子网都只能属于一个路由器)，默认只有一个 default 路由器，关联了所有子网 如果只想某一个子网的机器使用 NAT 网关，就新建一个路由器，关联上那个子网 点击路由器进入详情页面 新建路由策略 目的端：0.0.0.0/0 下一跳类型：NAT 网关 下一跳：新建的 NAT 网关 id © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"tke-skill/install-metrics-server-on-tke.html":{"url":"tke-skill/install-metrics-server-on-tke.html","title":"在 TKE 中安装 metrics-server","keywords":"","body":"在 TKE 中安装 metrics-server 下载 metrics-server repo:git clone https://github.com/kubernetes-incubator/metrics-server.git 修改 metrics-server 启动参数：--kubelet-insecure-tls ，防止 metrics server 访问 kubelet 采集指标时报证书问题(x509: certificate signed by unknown authority)， 在 deploy/1.8+/metrics-server-deployment.yaml 中加 args: containers: - name: metrics-server image: ccr.ccs.tencentyun.com/mirrors/metrics-server-amd64:v0.3.1 args: [\"--kubelet-insecure-tls\"] # 这里是新增的一行 imagePullPolicy: Always 在项目根目录执行:kubectl apply -f deploy/1.8+/ 注意是 apply 不是 create，apply 可以替换 kube-system 下的 apiservice，让 metric api 指向这个 metrics-server 等待一小段时间(确保 metrics-server 采集到了 node 和 pod 的 metrics 指标数据)，通过下面的命令检验一下:kubectl top pod --all-namespaces kubectl top node © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"tke-skill/tke-rbac.html":{"url":"tke-skill/tke-rbac.html","title":"RBAC细化集群的操作权限","keywords":"","body":"RBAC细化集群的操作权限 创建serviceAccount 就是创建一个ServiceAccount资源对象，同步一个样例： apiVersion: v1 kind: ServiceAccount metadata: name: hale namespace: default 创建之后，集群会自动给ServiceAccount绑一个secret，先记一下，后面有用。 $ kubectl get ServiceAccount hale -o jsonpath='{.secrets[0].name}' | xargs kubectl get secret -o yaml apiVersion: v1 data: ca.crt: xxxxxxx namespace: ZGVmYXVsdA== token: xxxxxxx kind: Secret metadata: annotations: kubernetes.io/service-account.name: hale kubernetes.io/service-account.uid: 99fb73b9-c273-11e9-8275-129ae4c231d4 creationTimestamp: 2019-08-19T11:22:17Z name: hale-token-5crhj namespace: default resourceVersion: \"89912726\" selfLink: /api/v1/namespaces/default/secrets/hale-token-5crhj uid: 99fee546-c273-11e9-8275-129ae4c231d4 type: kubernetes.io/service-account-token 配置RBAC授权 1 Role或者ClusterRole 用于配置具体的角色，一个角色代表了一组权限。根据生效的范围又分为Role（只在某个命名空间内有效）和ClusterRole（整个kubernetes集群范围内都有效）。比如授予对所有命名空间中的secret的读访问权限配置： kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: # 鉴于ClusterRole是集群范围对象，所以这里不需要定义\"namespace\"字段 name: secret-reader rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"get\", \"watch\", \"list\"] 2 RoleBinding或者ClusterRoleBinding 用于将角色和ServiceAccount绑定。同样，根据生效的范围又分为RoleBinding 和ClusterRoleBinding，其中，RoleBinding可以引用在同一命名空间下的Role对象，也可以引用一个ClusterRole；而ClusterRoleBinding只能引用ClusterRole对象。比如将ServiceAccount对象hale，绑一个ClusterRole权限（secret-reader）： kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: read-secret namespace: kube-system subjects: - kind: ServiceAccount name: hale namespace: default roleRef: kind: ClusterRole name: secret-reader apiGroup: rbac.authorization.k8s.io 更多参考：https://kubernetes.io/docs/reference/access-authn-authz/rbac/ TKE开启公网访问或者内网访问 在 “基本信息” 中，单击 “集群凭证” 中的 【显示凭证】。 开启公网访问（需要配置来源网段）或者内网访问 客户端配置kubeconfig # 配置user entry kubectl config set-credentials --token=bearer_token # 配置cluster entry kubectl config set-cluster --server=https://cls-xxx.ccs.tencent-cloud.com --certificate-authority=ca.crt # 配置context entry kubectl config set-context --cluster= --user= # 查看 kubectl config view # 配置当前使用的context kubectl config use-context 备注： Bearer_token: 来自于创建serviceAccount中secret的token字段，并base64解码。 ca.crt：来自于创建serviceAccount中secret的ca.crt字段，并base64解码（保存为文件ca.crt)。或者从集群凭证中直接拷贝“集群CA证书”。 © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"tke/why-nfs-10g.html":{"url":"tke/why-nfs-10g.html","title":"挂载的nfs为何是10G","keywords":"","body":"挂载的nfs为何是10G 挂载方式 集群创建的时候在控制台上设置 挂载成功之后，容器和node节点看到的大小都为10G 具体原因 nfs的盘，默认的挂载大小为10G，支持自动扩容 扩容方式 1T,到80%触发自动扩容。每次加500G 收费方式 计费是根据实际的使用量，小于10G不收费，多于10G开始收费 © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"tke/why-controller-manager-and-scheduler-unhealthy.html":{"url":"tke/why-controller-manager-and-scheduler-unhealthy.html","title":"为什么 controller-manager 和 scheduler 状态显示 Unhealthy","keywords":"","body":"为什么 controller-manager 和 scheduler 状态显示 Unhealthy kubectl get cs NAME STATUS MESSAGE ERROR scheduler Unhealthy Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: getsockopt: connection refused controller-manager Unhealthy Get http://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: getsockopt: connection refused etcd-0 Healthy {\"health\": \"true\"} 查看组件状态发现 controller-manager 和 scheduler 状态显示 Unhealthy，但是集群正常工作，是因为TKE metacluster托管方式集群的 apiserver 与 controller-manager 和 scheduler 不在同一个节点导致的，这个不影响功能。如果发现是 Healthy 说明 apiserver 跟它们部署在同一个节点，所以这个取决于部署方式。 更详细的原因： apiserver探测controller-manager 和 scheduler写死直接连的本机 func (s componentStatusStorage) serversToValidate() map[string]*componentstatus.Server { serversToValidate := map[string]*componentstatus.Server{ \"controller-manager\": {Addr: \"127.0.0.1\", Port: ports.InsecureKubeControllerManagerPort, Path: \"/healthz\"}, \"scheduler\": {Addr: \"127.0.0.1\", Port: ports.InsecureSchedulerPort, Path: \"/healthz\"}, } 源码：https://github.com/kubernetes/kubernetes/blob/v1.14.3/pkg/registry/core/rest/storage_core.go#L256 相关 issue: https://github.com/kubernetes/kubernetes/issues/19570 https://github.com/kubernetes/enhancements/issues/553 © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"troubleshooting/pod-terminating-forever.html":{"url":"troubleshooting/pod-terminating-forever.html","title":"Pod 状态一直 Terminating","keywords":"","body":"Pod 状态一直 Terminating 查看 Pod 事件: $ kubectl describe pod/apigateway-6dc48bf8b6-clcwk -n cn-staging Need to kill Pod Normal Killing 39s (x735 over 15h) kubelet, 10.179.80.31 Killing container with id docker://apigateway:Need to kill Pod 可能是磁盘满了，无法创建和删除 pod 处理建议是参考Kubernetes 最佳实践：处理容器数据磁盘被写满 DeadlineExceeded Warning FailedSync 3m (x408 over 1h) kubelet, 10.179.80.31 error determining status: rpc error: code = DeadlineExceeded desc = context deadline exceeded 怀疑是17版本dockerd的BUG。可通过 kubectl -n cn-staging delete pod apigateway-6dc48bf8b6-clcwk --force --grace-period=0 强制删除pod，但 docker ps 仍看得到这个容器 处置建议： 升级到docker 18. 该版本使用了新的 containerd，针对很多bug进行了修复。 如果出现terminating状态的话，可以提供让容器专家进行排查，不建议直接强行删除，会可能导致一些业务上问题。 存在 Finalizers k8s 资源的 metadata 里如果存在 finalizers，那么该资源一般是由某程序创建的，并且在其创建的资源的 metadata 里的 finalizers 加了一个它的标识，这意味着这个资源被删除时需要由创建资源的程序来做删除前的清理，清理完了它需要将标识从该资源的 finalizers 中移除，然后才会最终彻底删除资源。比如 Rancher 创建的一些资源就会写入 finalizers 标识。 处理建议：kubectl edit 手动编辑资源定义，删掉 finalizers，这时再看下资源，就会发现已经删掉了 © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"troubleshooting/pod-containercreating-forever.html":{"url":"troubleshooting/pod-containercreating-forever.html","title":"Pod 状态一直 ContainerCreating","keywords":"","body":"Pod 状态一直 ContainerCreating 查看 Pod 事件 $ kubectl describe pod/apigateway-6dc48bf8b6-l8xrw -n cn-staging no space left on device ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedCreatePodSandBox 2m (x4307 over 16h) kubelet, 10.179.80.31 (combined from similar events): Failed create pod sandbox: rpc error: code = Unknown desc = failed to create a sandbox for pod \"apigateway-6dc48bf8b6-l8xrw\": Error response from daemon: mkdir /var/lib/docker/aufs/mnt/1f09d6c1c9f24e8daaea5bf33a4230de7dbc758e3b22785e8ee21e3e3d921214-init: no space left on device node上磁盘满了，无法创建和删除 pod，解决方法参考Kubernetes最佳实践：处理容器数据磁盘被写满 Error syncing pod 可能是节点的内存碎片化严重，导致无法创建pod signal: killed memory limit 单位写错，误将memory的limit单位像request一样设置为小 m，这个单位在memory不适用，应该用Mi或M，会被k8s识别成byte，所以pause容器一起来就会被 cgroup-oom kill 掉，导致pod状态一直处于ContainerCreating controller-manager 异常 查看 master 上 kube-controller-manager 状态，异常的话尝试重启 © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"troubleshooting/pod-pending-forever.html":{"url":"troubleshooting/pod-pending-forever.html","title":"Pod 状态一直 Pending","keywords":"","body":"Pod 状态一直 Pending 资源不够 通过 kubectl describe node 查看 node 资源情况，关注以下信息： Allocatable 表示此节点 k8s 能够申请的资源总和 Allocated resources 表示此节点已分配的资源 前者与后者相减，可得出剩余可申请的资源。如果这个值小于 pod 的 request，就不满足 pod 的资源要求，也就不会调度上去 资源够用，但是未被调度 node 不满足 pod 的 nodeSelector 或 affinity 检查 pod 是否有 nodeSelector 或 affinity（亲和性）的配置，如果有，可能是 node 不满足要求导致无法被调度 旧 pod 无法解挂 cbs 云盘 可能是 pod 之前在另一个节点，但之前节点或kubelet挂了，现在漂移到新的节点上，但是之前pod挂载了cbs云盘，而由于之前节点或kubelet挂了导致无法对磁盘进行解挂，pod 漂移到新的节点时需要挂载之前的cbs云盘，但由于磁盘未被之前的节点解挂，所以新的节点无法进行挂载导致pod一直pending。 解决方法：在腾讯云控制台找到对应的云主机或磁盘，手动对磁盘进行卸载，然后pod自动重启时就可以成功挂载了（也可以delete pod让它立即重新调度） 镜像无法下载 看下 pod 的 event，看下是否是因为网络原因无法下载镜像或者下载私有镜像给的 secret 不对 低版本 kube-scheduler 的 bug 可能是低版本 kube-scheduler 的 bug, 可以升级下调度器版本 © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"troubleshooting/node-notready.html":{"url":"troubleshooting/node-notready.html","title":"节点 NotReady","keywords":"","body":"节点 NotReady 查看 Node 事件: kubectl get node -o yaml 看看 ready 状态 提示网络有问题 网络的初始化是在Master中做的，一般都是Master问题 没有找到什么特殊信息 一般需要到节点上看看kubelet或者docker日志 附录 节点 NotReady 原因一般分为以下三种情况： node上报健康心跳超时，一般是kubelet有问题、docker有问题、节点卡死等，此时node会被kube-controller-manager设置为notReady kubelet主动上报不健康，从yaml和event中能看到原因，一般是磁盘空间满，内存满等，这种是使用的问题；磁盘满考虑是否是在容器里写文件到可写层了（没有写到挂载的外部磁盘，写到本机磁盘了），更详细的参考 Kubernetes 最佳实践: 处理容器数据磁盘被写满；如果是发生系统OOM，参考 Kubernetes 最佳实践：合理设置 request 和 limit 初始化的时候NotReady，一般是网络没初始化好或者 （1）的变种，比如kubelet没有起来等 © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"troubleshooting/service-cannot-resolve.html":{"url":"troubleshooting/service-cannot-resolve.html","title":"Service 无法解析","keywords":"","body":"Service 无法解析 检查 dns 服务是否正常(kube-dns或CoreDNS) kubelet 启动参数 --cluster-dns 可以看到 dns 服务的 cluster ip:$ ps -ef | grep kubelet ... /usr/bin/kubelet --cluster-dns=172.16.14.217 ... 找到 dns 的 service:$ kubectl get svc -n kube-system | grep 172.16.14.217 kube-dns ClusterIP 172.16.14.217 53/TCP,53/UDP 47d 看是否存在 endpoint:$ kubectl -n kube-system describe svc kube-dns | grep -i endpoints Endpoints: 172.16.0.156:53,172.16.0.167:53 Endpoints: 172.16.0.156:53,172.16.0.167:53 检查 endpoint 的 对应 pod 是否正常:$ kubectl -n kube-system get pod -o wide | grep 172.16.0.156 kube-dns-898dbbfc6-hvwlr 3/3 Running 0 8d 172.16.0.156 10.0.0.3 dns 服务正常，pod 与 dns 服务之间网络不通 检查 dns 服务运行正常，再检查下 pod 是否连不上 dns 服务，可以在 pod 里 telnet 一下 dns 的 53 端口:# 连 dns service 的 cluster ip $ telnet 172.16.14.217 53 如果检查到是网络不通，就需要排查下网络设置 检查节点的安全组设置，需要放开集群的容器网段 检查是否还有防火墙规则，检查 iptables © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"troubleshooting/lb-abnormal.html":{"url":"troubleshooting/lb-abnormal.html","title":"LB 显示异常","keywords":"","body":"LB 显示异常 通常是LB对绑定的后端节点进行健康检查探测时失败，我们来分析下原因 Service 的 targetPort 不正确 Service 定义中的 targetPort 对应容器的端口，如果容器监听的是 8080，而 targetPort 写的 80，那么会不通，所以确保 targetPort 与容器监听的端口要一致。 节点安全组没放开 NodePort 区间 由于 TKE 的 LoadBalancer Service 基于 NodePort 实现，所以 LB 会绑定节点的 NodePort 端口(Service中的nodePort字段)，也就是 LB 会将请求直接发到节点的 NodePort 端口上，然后 k8s 内部再通过 kube-proxy 将数据包路由到对应的 pod 中。 通常节点安全组都对内网放开访问的，所以如果是内网LB一般不会这样，如果LB是公网类型，它对绑定的节点进行健康检查探测的源ip就是这个公网ip，而nodePort区间是30000-32768，所以节点安全组应该对 0.0.0.0/0 的 TCP 和 UDP 设为允许。 © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"troubleshooting/cannot-visit-internet.html":{"url":"troubleshooting/cannot-visit-internet.html","title":"集群内无法访问外网","keywords":"","body":"集群内无法访问外网 首先确保 node 能上外网(node 本身有公网还是走 NAT 网关)，如果在node上都不能访问外网，则集群内也无法访问 TKE 1.10 以上的集群在 kube-system 下有 ip-masq-agent 的 Deamonset，作用是将出公网的请求的源 ip snat 成 node 的 ip，这样数据包才能出公网，确保 node 上有 ip-masq-agent 的 pod: kubectl get pod -n kube-system -o wide 如果发现完全没有，请提工单 如果发现有，但部分没有，检查下节点是否设置了污点(taint)，导致 Deamonset 的 pod 无法被调度上来 如果发现都有，还是无法访问公网，检查下 ip-masq-agent 的 配置:kubectl -n kube-system get cm ip-masq-agent-config -o yaml nonMasqueradeCIDRs 里的网段表示对这些网段不做snat，出公网需要snat，看是否设置了类似 0.0.0.0/0 的网段导致出公网的数据包也没做snat © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"troubleshooting/cannot-visit-service-out-of-cluster.html":{"url":"troubleshooting/cannot-visit-service-out-of-cluster.html","title":"无法访问集群外的服务","keywords":"","body":"无法访问集群外的服务 比如，购买了 mysql, redis 等外部服务，或者在集群外自己搭建了一些服务(如自建dns、暂未容器化的服务等），集群内的pod访问不通这些服务 原因 通常是安全组问题，容器网络是在 vpc 层面实现的，pod ip 在整个 vpc 内都可以路由，如果容器访问 vpc 内但在集群外的服务，需要这个服务对应机器的安全组放开这个容器网段，因为服务收到容器的数据报文的源IP就是pod ip © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"troubleshooting/pod-cannot-exec-or-logs.html":{"url":"troubleshooting/pod-cannot-exec-or-logs.html","title":"Pod 无法被 exec 和 logs","keywords":"","body":"Pod 无法被 exec 和 logs 通常是 apiserver --> kubelet:10250 之间的网络不通，10250 是 kubelet 提供接口的端口，kubectl exec和kubectl logs 的原理就是 apiserver 调 kubelet，kubelet 再调 dockerd 来实现的，所以要保证 kubelet 10250 端口对 apiserver 放通。 TKE托管集群通常不会出现此情况，master 不受节点安全组限制 如果是TKE独立集群，检查节点安全组是否对master节点放通了 10250 端口，如果没放通会导致 apiserver 无法访问 kubelet 10250 端口，从而导致无法进入容器或查看log(kubectl exec和kubectl logs) 检查防火墙、iptables规则是否对 10250 端口数据包进行了拦截 © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"troubleshooting/cannot-delete-job.html":{"url":"troubleshooting/cannot-delete-job.html","title":"Job 无法被删除","keywords":"","body":"Job 无法被删除 原因 可能是 k8s 的一个bug: https://github.com/kubernetes/kubernetes/issues/43168 本质上是脏数据问题，Running+Succeed != 期望Completions 数量，低版本 kubectl 不容忍，delete job 的时候打开debug(加-v=8)，会看到kubectl不断在重试，直到达到timeout时间。新版kubectl会容忍这些，删除job时会删除关联的pod 解决方法 升级 kubectl 版本，1.12 以上 低版本 kubectl 删除 job 时带 --cascade=false 参数(如果job关联的pod没删完，加这个参数不会删除关联的pod)kubectl delete job --cascade=false © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"troubleshooting/service-cannot-be-visited.html":{"url":"troubleshooting/service-cannot-be-visited.html","title":"服务不能被访问","keywords":"","body":"服务不能被访问 targetPort 错误 现象：node上直接访问pod能通，访问 service 和 nodePort 不行 原因：service 的 targetPort 与容器实际监听端口不一致 解决方法：更正 service 的 targetPort k8s 支持 ipvs 的 bug 现象： node上直接访问pod能通，访问 service 不行且集群开启了 ipvs、集群版本小于1.11、node上有pod使用了与service同端口号的hostPort 原因：是 k8s 对 ipvs 支持的 bug，1.11 版本修复 参考： https://github.com/kubernetes/kubernetes/issues/66103 https://github.com/kubernetes/kubernetes/issues/60688 http://tapd.oa.com/qcloud_docker/prong/stories/view/1010140411063954339 解决方法：使用不与 service 端口号冲突的 hostPort 或升级集群版本 安全组没放通容器网段 node的安全组需要放通容器网段，因为访问 service 时，数据包可能从一个node转发到另一个node，源IP网段是容器网段，如果node安全组没放通转发就会失败 © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"istio/cannot-visit-out-of-mesh.html":{"url":"istio/cannot-visit-out-of-mesh.html","title":"网格内部无法访问外部","keywords":"","body":"网格内部无法访问外部 ISTIO 用法问题 istio 默认只跟网格内部可以互通，要访问外部要用 ServiceEntry 是来注册外部服务到网格；一般用 VirtualService 表示内部服务，VirtualService 要转发请求到外部服务，destination 可以写成 ServiceEntry 里声明的 host ServiceEntry 的 addresses 是用来限制请求报文中的目标 ip 范围的，而不是外部服务的地址，endpoints 字段才是 安全组设置问题 如果是访问集群外，局域网内的服务，可能是安全组设置问题，参考：无法访问集群外的服务 ip-masq-agent 问题 如果是访问的公网，可能是 ip-masq-agent 的问题，参考: 集群内无法访问外网 © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"damn/k8s-ipvs-bug.html":{"url":"damn/k8s-ipvs-bug.html","title":"K8S 对 IPVS 支持的 BUG","keywords":"","body":"K8S 对 IPVS 支持的 BUG 使用 hostPort 导致所在节点的pod访问不到同端口号的 service 启用 ipvs 的集群，如果某 pod 使用 hostPort 监听端口，那么这个 pod 所在的节点访问跟 hostPort 同端口号的任何 Service 都会被路由到这个 hostPort 上，而不会正确路由到 Service 对应的后端 pod 原因：ipvs 和 iptables 都会向 netfilter 的 hook 点插入处理函数，由于 hostPort 是通过 iptables 实现的，所以当某节点的 pod 使用了 hostPort 暴露端口，就会写入 iptables 规则，让目标端口为该 hostPort 端口号的报文全部路由到该 pod；当启用 ipvs 来路由 service 时会写入 ipvs 规则，但写入的 ipvs 规则和 iptables 规则进入 netfilter 的 hook 点时，是有顺序的，netfilter 会先按顺序执行这些 hook 函数，iptables 规则的 hook 先被执行就会导致这个现象：该节点 pod 访问某个 service 的某个端口号，如果这个端口号跟 hostPort 一致，数据包就会被路由到 hostPort 对应的 pod 里 规避方案：用其它不跟任何 service 端口冲突的端口作为 hostPort 来绕开此问题 Github 相关 Issue： https://github.com/kubernetes/kubernetes/issues/66103 https://github.com/kubernetes/kubernetes/issues/60688 暂未验证新版是否彻底修复此 bug，感兴趣的同学可以验证下提 PR 到这里说明下 无法使⽤ localhost+nodeport 启用ipvs的情况下，使用localhost无法访问本机nodeport，这是一个已知bug，k8s本身也不会这样用，通常只有测试访问本机nodeport才会遇到这个问题，不影响生产环境的服务 相关issue: https://github.com/kubernetes/kubernetes/issues/67730 所有节点的kube-proxy不能为新的service创建路由规则 现象：是 kube-proxy 的 Netlink deadlock 的 bug 导致 issue: https://github.com/kubernetes/kubernetes/issues/71071 1.14 版本已修复，修复的 PR: https://github.com/kubernetes/kubernetes/pull/72361 © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"damn/lost-packets-once-enable-tcp-tw-recycle.html":{"url":"damn/lost-packets-once-enable-tcp-tw-recycle.html","title":"开启tcp_tw_recycle内核参数在NAT环境会丢包","keywords":"","body":"开启tcp_tw_recycle内核参数在NAT环境会丢包 原因 tcp_tw_recycle参数。它用来快速回收TIME_WAIT连接，不过如果在NAT环境下会引发问题。 RFC1323中有如下一段描述： An additional mechanism could be added to the TCP, a per-host cache of the last timestamp received from any connection. This value could then be used in the PAWS mechanism to reject old duplicate segments from earlier incarnations of the connection, if the timestamp clock can be guaranteed to have ticked at least once since the old connection was open. This would require that the TIME-WAIT delay plus the RTT together must be at least one tick of the sender’s timestamp clock. Such an extension is not part of the proposal of this RFC. 大概意思是说TCP有一种行为，可以缓存每个连接最新的时间戳，后续请求中如果时间戳小于缓存的时间戳，即视为无效，相应的数据包会被丢弃。 Linux是否启用这种行为取决于tcp_timestamps和tcp_tw_recycle，因为tcp_timestamps缺省就是开启的，所以当tcp_tw_recycle被开启后，实际上这种行为就被激活了，当客户端或服务端以NAT方式构建的时候就可能出现问题，下面以客户端NAT为例来说明： 当多个客户端通过NAT方式联网并与服务端交互时，服务端看到的是同一个IP，也就是说对服务端而言这些客户端实际上等同于一个，可惜由于这些客户端的时间戳可能存在差异，于是乎从服务端的视角看，便可能出现时间戳错乱的现象，进而直接导致时间戳小的数据包被丢弃。如果发生了此类问题，具体的表现通常是是客户端明明发送的SYN，但服务端就是不响应ACK。 在4.12之后的内核已移除tcp_tw_recycle内核参数: https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=4396e46187ca5070219b81773c4e65088dac50cc https://github.com/torvalds/linux/commit/4396e46187ca5070219b81773c4e65088dac50cc TKE中 使用 NAT 的场景 跨 VPC 访问(通过对等连接、云联网、专线等方式打通)，会做 SNAT © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"damn/dns-lookup-delay.html":{"url":"damn/dns-lookup-delay.html","title":"部分 DNS 查询延迟的原因与解决方案","keywords":"","body":"部分 DNS 查询延迟的原因与解决方案 本文摘自腾讯云容器团队博文: kubernetes集群中夺命的5秒DNS延迟 超时问题 客户反馈从pod中访问服务时，总是有些请求的响应时延会达到5秒。正常的响应只需要毫秒级别的时延。 DNS 5秒延时 在pod中(通过nsenter -n tcpdump)抓包，发现是有的DNS请求没有收到响应，超时5秒后，再次发送DNS请求才成功收到响应。 在kube-dns pod抓包，发现是有DNS请求没有到达kube-dns pod， 在中途被丢弃了。 为什么是5秒？ man resolv.conf可以看到glibc的resolver的缺省超时时间是5s。 丢包原因 经过搜索发现这是一个普遍问题。 根本原因是内核conntrack模块的bug，netfilter做NAT时可能发生资源竞争导致部分报文丢弃。 Weave works的工程师Martynas Pumputis对这个问题做了很详细的分析： https://www.weave.works/blog/racy-conntrack-and-dns-lookup-timeouts 相关结论： 只有多个线程或进程，并发从同一个socket发送相同五元组的UDP报文时，才有一定概率会发生 glibc, musl(alpine linux的libc库)都使用\"parallel query\", 就是并发发出多个查询请求，因此很容易碰到这样的冲突，造成查询请求被丢弃 由于ipvs也使用了conntrack, 使用kube-proxy的ipvs模式，并不能避免这个问题 问题的根本解决 Martynas向内核提交了两个patch来fix这个问题，不过他说如果集群中有多个DNS server的情况下，问题并没有完全解决。 其中一个patch已经在2018-7-18被合并到linux内核主线中: netfilter: nf_conntrack: resolve clash for matching conntracks 目前只有4.19.rc 版本包含这个patch。 规避办法 规避方案一：使用TCP发送DNS请求 由于TCP没有这个问题，有人提出可以在容器的resolv.conf中增加options use-vc, 强制glibc使用TCP协议发送DNS query。下面是这个man resolv.conf中关于这个选项的说明： use-vc (since glibc 2.14) Sets RES_USEVC in _res.options. This option forces the use of TCP for DNS resolutions. 笔者使用镜像\"busybox:1.29.3-glibc\" (libc 2.24) 做了试验，并没有见到这样的效果，容器仍然是通过UDP发送DNS请求。 规避方案二：避免相同五元组DNS请求的并发 resolv.conf还有另外两个相关的参数： single-request-reopen (since glibc 2.9) single-request (since glibc 2.10) man resolv.conf中解释如下： single-request-reopen (since glibc 2.9) Sets RES_SNGLKUPREOP in _res.options. The resolver uses the same socket for the A and AAAA requests. Some hardware mistakenly sends back only one reply. When that happens the client system will sit and wait for the second reply. Turning this option on changes this behavior so that if two requests from the same port are not handled correctly it will close the socket and open a new one before sending the second request. single-request (since glibc 2.10) Sets RES_SNGLKUP in _res.options. By default, glibc performs IPv4 and IPv6 lookups in parallel since version 2.9. Some appliance DNS servers cannot handle these queries properly and make the requests time out. This option disables the behavior and makes glibc perform the IPv6 and IPv4 requests sequentially (at the cost of some slowdown of the resolving process). 笔者做了试验，发现效果是这样的： single-request-reopen 发送A类型请求和AAAA类型请求使用不同的源端口。这样两个请求在conntrack表中不占用同一个表项，从而避免冲突。 single-request 避免并发，改为串行发送A类型和AAAA类型请求。没有了并发，从而也避免了冲突。 要给容器的resolv.conf加上options参数，有几个办法： 1) 在容器的\"ENTRYPOINT\"或者\"CMD\"脚本中，执行/bin/echo 'options single-request-reopen' >> /etc/resolv.conf 2) 在pod的postStart hook中： lifecycle: postStart: exec: command: - /bin/sh - -c - \"/bin/echo 'options single-request-reopen' >> /etc/resolv.conf\" 3) 使用template.spec.dnsConfig (k8s v1.9 及以上才支持): template: spec: dnsConfig: options: - name: single-request-reopen 4) 使用ConfigMap覆盖POD里面的/etc/resolv.conf configmap: apiVersion: v1 data: resolv.conf: | nameserver 1.2.3.4 search default.svc.cluster.local svc.cluster.local cluster.local ec2.internal options ndots:5 single-request-reopen timeout:1 kind: ConfigMap metadata: name: resolvconf POD spec: volumeMounts: - name: resolv-conf mountPath: /etc/resolv.conf subPath: resolv.conf ... volumes: - name: resolv-conf configMap: name: resolvconf items: - key: resolv.conf path: resolv.conf 5) 使用MutatingAdmissionWebhook MutatingAdmissionWebhook 是1.9引入的Controller，用于对一个指定的Resource的操作之前，对这个resource进行变更。 istio的自动sidecar注入就是用这个功能来实现的。 我们也可以通过MutatingAdmissionWebhook，来自动给所有POD，注入以上3)或者4)所需要的相关内容。 以上方法中， 1)和2)都需要修改镜像， 3)和4)则只需要修改POD的spec， 能适用于所有镜像。不过还是有不方便的地方： 每个工作负载的yaml都要做修改，比较麻烦 对于通过helm创建的工作负载，需要修改helm charts 方法5)对集群使用者最省事，照常提交工作负载即可。不过初期需要一定的开发工作量。 规避方案三：使用本地DNS缓存 容器的DNS请求都发往本地的DNS缓存服务(dnsmasq, nscd等)，不需要走DNAT，也不会发生conntrack冲突。另外还有个好处，就是避免DNS服务成为性能瓶颈。 使用本地DNS缓存有两种方式： 每个容器自带一个DNS缓存服务 每个节点运行一个DNS缓存服务，所有容器都把本节点的DNS缓存作为自己的nameserver 从资源效率的角度来考虑的话，推荐后一种方式。 实施办法 条条大路通罗马，不管怎么做，最终到达上面描述的效果即可。 POD中要访问节点上的DNS缓存服务，可以使用节点的IP。 如果节点上的容器都连在一个虚拟bridge上， 也可以使用这个bridge的三层接口的IP(在TKE中，这个三层接口叫cbr0)。 要确保DNS缓存服务监听这个地址。 如何把POD的/etc/resolv.conf中的nameserver设置为节点IP呢？ 一个办法，是设置 POD.spec.dnsPolicy 为 \"Default\"， 意思是POD里面的 /etc/resolv.conf， 使用节点上的文件。缺省使用节点上的 /etc/resolv.conf(如果kubelet通过参数--resolv-conf指定了其他文件，则使用--resolv-conf所指定的文件)。 另一个办法，是给每个节点的kubelet指定不同的--cluster-dns参数，设置为节点的IP，POD.spec.dnsPolicy仍然使用缺省值\"ClusterFirst\"。 kops项目甚至有个issue在讨论如何在部署集群时设置好--cluster-dns指向节点IP: https://github.com/kubernetes/kops/issues/5584 参考资料 Racy conntrack and DNS lookup timeouts: https://www.weave.works/blog/racy-conntrack-and-dns-lookup-timeouts 5 – 15s DNS lookups on Kubernetes? : https://blog.quentin-machu.fr/2018/06/24/5-15s-dns-lookups-on-kubernetes/ DNS intermittent delays of 5s: https://github.com/kubernetes/kubernetes/issues/56903 记一次Docker/Kubernetes上无法解释的连接超时原因探寻之旅: https://mp.weixin.qq.com/s/VYBs8iqf0HsNg9WAxktzYQ © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"damn/rancher-remove-node.html":{"url":"damn/rancher-remove-node.html","title":"Rancher 清除 Node 导致集群异常","keywords":"","body":"Rancher 清除 Node 导致集群异常 现象 安装了 rancher 的用户，在卸载 rancher 的时候，可能会手动执行 kubectl delete ns local 来删除这个 rancher 创建的 namespace，但直接这样做会导致所有 node 被清除，通过 kubectl get node 获取不到 node，TKE 控制台也会显示这个集群节点全部异常。 原因 看了下rancher源码，rancher通过nodes.management.cattle.io这个CRD存储和管理node，会给所有node创建对应的这个CRD资源，metadata中加入了两个finalizer，其中\"user-node-remove_local\"对应的处理逻辑就是删除对应的k8s node资源，也就是 delete ns local 时，会尝试删除 nodes.management.cattle.io 这些CRD资源，进而触发 rancher 的 finalizer 逻辑去删除 对应的 k8s node 资源，从而清空了 node，所以kubectl get node就看不到node了，tke发现已添加的节点通过k8s api 全都无法 get 了，就显示全部异常。 规避方案 不要在rancher组件卸载完之前手动delete ns local © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"skill/analysis-exitcode.html":{"url":"skill/analysis-exitcode.html","title":"分析 ExitCode 定位程序退出原因","keywords":"","body":"分析 ExitCode 定位程序退出原因 使用 kubectl describe pod 查看异常的 pod 的状态，在容器列表里看 State 字段，其中 ExitCode 即程序退出时的状态码，正常退出时为0。如果不为0，表示异常退出，我们可以分析下原因。 退出状态码的区间 必须在 0-255 之间 0 表示正常退出 外界中断将程序退出的时候状态码区间在 129-255，(操作系统给程序发送中断信号，比如 kill -9 是 SIGKILL，ctrl+c 是 SIGINT) 一般程序自身原因导致的异常退出状态区间在 1-128 (这只是一般约定，程序如果一定要用129-255的状态码也是可以的) 假如写代码指定的退出状态码时不在 0-255 之间，例如: exit(-1)，这时会自动做一个转换，最终呈现的状态码还是会在 0-255 之间。我们把状态码记为 code 当指定的退出时状态码为负数，那么转换公式如下: 256 - (|code| % 256) 当指定的退出时状态码为正数，那么转换公式如下: code % 256 常见异常状态码 137 此状态码一般是因为 pod 中容器内存达到了它的资源限制(resources.limits)，一般是内存溢出(OOM)，CPU达到限制只需要不分时间片给程序就可以。因为限制资源是通过 linux 的 cgroup 实现的，所以 cgroup 会将此容器强制杀掉，类似于 kill -9 还可能是宿主机本身资源不够用了(OOM)，内核会选取一些进程杀掉来释放内存 不管是 cgroup 限制杀掉进程还是因为节点机器本身资源不够导致进程死掉，都可以从系统日志中找到记录: ubuntu 的系统日志在 /var/log/syslog，centos 的系统日志在 /var/log/messages，都可以用 journalctl -k 来查看系统日志 1 和 255 这种可能是一般错误，具体错误原因只能看容器日志，因为很多程序员写异常退出时习惯用 exit(1) 或 exit(-1)，-1 会根据转换规则转成 255 状态码参考 这里罗列了一些状态码的含义：Appendix E. Exit Codes With Special Meanings Linux 标准中断信号 Linux 程序被外界中断时会发送中断信号，程序退出时的状态码就是中断信号值加上 128 得到的，比如 SIGKILL 的中断信号值为 9，那么程序退出状态码就为 9+128=137。以下是标准信号值参考： Signal Value Action Comment ────────────────────────────────────────────────────────────────────── SIGHUP 1 Term Hangup detected on controlling terminal or death of controlling process SIGINT 2 Term Interrupt from keyboard SIGQUIT 3 Core Quit from keyboard SIGILL 4 Core Illegal Instruction SIGABRT 6 Core Abort signal from abort(3) SIGFPE 8 Core Floating-point exception SIGKILL 9 Term Kill signal SIGSEGV 11 Core Invalid memory reference SIGPIPE 13 Term Broken pipe: write to pipe with no readers; see pipe(7) SIGALRM 14 Term Timer signal from alarm(2) SIGTERM 15 Term Termination signal SIGUSR1 30,10,16 Term User-defined signal 1 SIGUSR2 31,12,17 Term User-defined signal 2 SIGCHLD 20,17,18 Ign Child stopped or terminated SIGCONT 19,18,25 Cont Continue if stopped SIGSTOP 17,19,23 Stop Stop process SIGTSTP 18,20,24 Stop Stop typed at terminal SIGTTIN 21,21,26 Stop Terminal input for background process SIGTTOU 22,22,27 Stop Terminal output for background process C/C++ 退出状态码 /usr/include/sysexits.h 试图将退出状态码标准化(仅限 C/C++): #define EX_OK 0 /* successful termination */ #define EX__BASE 64 /* base value for error messages */ #define EX_USAGE 64 /* command line usage error */ #define EX_DATAERR 65 /* data format error */ #define EX_NOINPUT 66 /* cannot open input */ #define EX_NOUSER 67 /* addressee unknown */ #define EX_NOHOST 68 /* host name unknown */ #define EX_UNAVAILABLE 69 /* service unavailable */ #define EX_SOFTWARE 70 /* internal software error */ #define EX_OSERR 71 /* system error (e.g., can't fork) */ #define EX_OSFILE 72 /* critical OS file missing */ #define EX_CANTCREAT 73 /* can't create (user) output file */ #define EX_IOERR 74 /* input/output error */ #define EX_TEMPFAIL 75 /* temp failure; user is invited to retry */ #define EX_PROTOCOL 76 /* remote error in protocol */ #define EX_NOPERM 77 /* permission denied */ #define EX_CONFIG 78 /* configuration error */ #define EX__MAX 78 /* maximum listed value */ © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"skill/capture-packets-in-container.html":{"url":"skill/capture-packets-in-container.html","title":"容器内抓包定位网络问题","keywords":"","body":"容器内抓包定位网络问题 在使用 kubernetes 跑应用的时候，可能会遇到一些网络问题，比较常见的是服务端无响应(超时)或回包内容不正常，如果没找出各种配置上有问题，这时我们需要确认数据包到底有没有最终被路由到容器里，或者报文到达容器的内容和出容器的内容符不符合预期，通过分析报文可以进一步缩小问题范围。那么如何在容器内抓包呢？本文提供实用的脚本一键进入容器网络命名空间(netns)，使用宿主机上的tcpdump进行抓包。 使用脚本一键进入 pod netns 抓包 发现某个服务不通，最好将其副本数调为1，并找到这个副本 pod 所在节点和 pod 名称 kubectl get pod -o wide 登录 pod 所在节点，将如下脚本粘贴到 shell (注册函数到当前登录的 shell，我们后面用) function e() { set -eu ns=${2-\"default\"} pod=`kubectl -n $ns describe pod $1 | grep -A10 \"^Containers:\" | grep -Eo 'docker://.*$' | head -n 1 | sed 's/docker:\\/\\/\\(.*\\)$/\\1/'` pid=`docker inspect -f {{.State.Pid}} $pod` echo \"entering pod netns for $ns/$1\" cmd=\"nsenter -n --target $pid\" echo $cmd $cmd } 一键进入 pod 所在的 netns，格式：e POD_NAME NAMESPACE，示例： e istio-galley-58c7c7c646-m6568 istio-system e proxy-5546768954-9rxg6 # 省略 NAMESPACE 默认为 default 这时已经进入 pod 的 netns，可以执行宿主机上的 ip a 或 ifconfig 来查看容器的网卡，执行 netstat -tunlp 查看当前容器监听了哪些端口，再通过 tcpdump 抓包： tcpdump -i eth0 -w test.pcap port 80 ctrl-c 停止抓包，再用 scp 或 sz 将抓下来的包下载到本地使用 wireshark 分析，提供一些常用的 wireshark 过滤语法： # 使用 telnet 连上并发送一些测试文本，比如 \"lbtest\"， # 用下面语句可以看发送的测试报文有没有到容器 tcp contains \"lbtest\" # 如果容器提供的是http服务，可以使用 curl 发送一些测试路径的请求， # 通过下面语句过滤 uri 看报文有没有都容器 http.request.uri==\"/mytest\" 脚本原理 我们解释下步骤二中用到的脚本的原理 查看指定 pod 运行的容器 ID kubectl describe pod -n mservice 获得容器进程的 pid docker inspect -f {{.State.Pid}} 进入该容器的 network namespace nsenter -n --target 依赖宿主机的命名：kubectl, docker, nsenter, grep, head, sed © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"skill/analysis-docker-disk.html":{"url":"skill/analysis-docker-disk.html","title":"分析docker磁盘占用","keywords":"","body":"分析docker磁盘占用 有时候磁盘占用高但不知道被什么占用了，我们可以来分析下。 通过 docker system df 看磁盘主要被哪部分占用(镜像、容器、volume) local volume 占用高 docker system df -v 查看哪些 volume 占用高 根据 volume id 反查容器: docker ps -a --filter volume=5637d54ece7b02cfbbaa5d901db72f45a72802b77f5ec5b3bf996496606854ea docker inspect 一下容器，看被挂载的 volume 的挂载点是哪里，这里假设是 /data docker inspect 一下对应的镜像，里面应该也是声明了 volume 挂载点是 /data（Dockerfile 声明了挂载点 volume，如果启动容器并没有挂载外部数据卷到声明的挂载点，docker默认会在宿主机创建 /var/lib/docker/volumes/ 目录，并将其子目录 _data 挂载到容器里，而不是直接写到可写层 /var/lib/docker/aufs/diff/ 下面） 通过 k8s_es-data_es-data-2_default_428632b6-3bc6-11e9-8843-0a587f8023f1_0 可分析这是 default 命名空间下 es-data-2 的pod所用到的容器 反查根源 kubectl describe 或 kubectl get -o yaml 看下检查volume挂载点，如果是挂了外部数据卷，看是否是没有挂载到 Dockerfile 声明的 volume 挂载点 /data 下，如果挂错了就纠正挂载路径 © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"skill/analysis-cidr.html":{"url":"skill/analysis-cidr.html","title":"分析网络划分计算最大节点、service 与 pod数量","keywords":"","body":"分析网络划分计算最大节点、service 与 pod数量 看 controller-manager 启动参数$ ps -ef | grep kube-controller-manager /usr/bin/kube-controller-manager --cluster-cidr=10.99.0.0/19 --service-cluster-ip-range=10.99.28.0/22 ... --cluster-cidr=10.99.0.0/19 表示集群网络的 CIDR --service-cluster-ip-range=10.99.28.0/22 表示 Service 占用的子网(在TKE中是属于集群网络CIDR范围内的一个子网) TKE 默认每个节点的 CIDR 是 24 位，可以通过 kubectl describe node 查看 PodCIDR 字段来看，这里假设实际就是 24 位 此例中集群 Service 数量为：2^(32-22)=1024 个。公式：2 ^ (32 - SERVICE_CIDR_MASK_SIZE) 此例中集群节点最大数量：2^(24-19) - 2^(24-22) = 32 - 4 = 28 个 (Service占用4个节点子网段) 公式：2 ^ (POD_CIDR_MASK_SIZE - CLUSTER_CIDR_MASK_SIZE) - 2 ^ (POD_CIDR_MASK_SIZE - SERVICE_CIDR_MASK_SIZE) 此例中每个节点可以容纳 2^(32-24)=256 个 IP，减去网络地址、广播地址和子网为1的网桥 IP 地址(cbr0)，每个节点最多可以容纳 253 个 pod。但是节点 pod 实际最大容量还需要看 kubelet 启动参数 --max-pods 的值。通过 kubectl describe node 也能看到节点最大 pod 数 (Capacity.pods)。节点最大 pod 数计算公式：2 ^ (32 - POD_CIDR_MASK_SIZE) - 3 © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"best-practice/kubernetes-best-practice-grace-update.html":{"url":"best-practice/kubernetes-best-practice-grace-update.html","title":"优雅热更新","keywords":"","body":"优雅热更新 当kubernetes对服务滚动更新的期间，默认配置的情况下可能会让部分连接异常（比如连接被拒绝），我们来分析下原因并给出最佳实践 滚动更新场景 使用 deployment 部署服务并关联 service 修改 deployment 的 replica 调整副本数量来滚动更新 升级程序版本(修改镜像tag)触发 deployment 新建 replicaset 启动新版本的 pod 使用 HPA (HorizontalPodAutoscaler) 来对 deployment 自动扩缩容 更新过程连接异常的原因 滚动更新时，service 对应的 pod 会被创建或销毁，也就是 service 对应的 endpoint 列表会新增或移除endpoint，更新期间可能让部分连接异常，主要原因是： pod 被创建，还没完全启动就被 endpoint controller 加入到 service 的 endpoint 列表，然后 kube-proxy 配置对应的路由规则(iptables/ipvs)，如果请求被路由到还没完全启动完成的 pod，这时 pod 还不能正常处理请求，就会导致连接异常 pod 被销毁，但是从 endpoint controller watch 到变化并更新 service 的 endpoint 列表到 kube-proxy 更新路由规则这期间有个时间差，pod可能已经完全被销毁了，但是路由规则还没来得及更新，造成请求依旧还能被转发到已经销毁的 pod ip，导致连接异常 最佳实践 针对第一种情况，可以给 pod 里的 container 加 readinessProbe (就绪检查)，这样可以让容器完全启动了才被endpoint controller加进 service 的 endpoint 列表，然后 kube-proxy 再更新路由规则，这时请求被转发到的所有后端 pod 都是正常运行，避免了连接异常 针对第二种情况，可以给 pod 里的 container 加 preStop hook，让 pod 真正销毁前先 sleep 等待一段时间，留点时间给 endpoint controller 和 kube-proxy 清理 endpoint 和路由规则，这段时间 pod 处于 Terminating 状态，在路由规则更新完全之前如果有请求转发到这个被销毁的 pod，请求依然可以被正常处理，因为它还没有被真正销毁 最佳实践 yaml 示例: apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx spec: replicas: 1 selector: matchLabels: component: nginx template: metadata: labels: component: nginx spec: containers: - name: nginx image: \"nginx\" ports: - name: http hostPort: 80 containerPort: 80 protocol: TCP readinessProbe: httpGet: path: /healthz port: 80 httpHeaders: - name: X-Custom-Header value: Awesome initialDelaySeconds: 15 timeoutSeconds: 1 lifecycle: preStop: exec: command: [\"/bin/bash\", \"-c\", \"sleep 30\"] 参考资料 Container probes: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes Container Lifecycle Hooks: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/ © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"best-practice/kubernetes-best-practice-handle-disk-full.html":{"url":"best-practice/kubernetes-best-practice-handle-disk-full.html","title":"处理容器数据磁盘被写满","keywords":"","body":"处理容器数据磁盘被写满 容器数据磁盘被写满造成的危害: 不能创建 Pod (一直 ContainerCreating) 不能删除 Pod (一直 Terminating) 无法 exec 到容器 判断是否被写满: 容器数据目录大多会单独挂数据盘，路径一般是 /var/lib/docker，也可能是 /data/docker 或 /opt/docker，取决于节点被添加时的配置： 可通过 docker info 确定： $ docker info ... Docker Root Dir: /var/lib/docker ... 如果没有单独挂数据盘，则会使用系统盘存储。判断是否被写满： $ df Filesystem 1K-blocks Used Available Use% Mounted on ... /dev/vda1 51474044 4619112 44233548 10% / ... /dev/vdb 20511356 20511356 0 100% /var/lib/docker 解决方法 先恢复业务，清理磁盘空间 重启 dockerd (清理容器日志输出和可写层文件) 重启前需要稍微腾出一点空间，不然重启 docker 会失败，可以手动删除一些docker的log文件或可写层文件，通常删除log: $ cd /var/lib/docker/containers $ du -sh * # 找到比较大的目录 $ cd dda02c9a7491fa797ab730c1568ba06cba74cecd4e4a82e9d90d00fa11de743c $ cat /dev/null > dda02c9a7491fa797ab730c1568ba06cba74cecd4e4a82e9d90d00fa11de743c-json.log.9 # 删除log文件 注意: 使用 cat /dev/null > 方式删除而不用 rm，因为用 rm 删除的文件，docker 进程可能不会释放文件，空间也就不会释放；log 的后缀数字越大表示越久远，先删除旧日志。 将该 node 标记不可调度，并将其已有的 pod 驱逐到其它节点，这样重启dockerd就会让该节点的pod对应的容器删掉，容器相关的日志(标准输出)与容器内产生的数据文件(可写层)也会被清理： kubectl drain 10.179.80.31 重启 dockerd: systemctl restart dockerd 取消不可调度的标记: kubectl uncordon 10.179.80.31 定位根因，彻底解决 问题定位方法见附录，这里列举根因对应的解决方法： 日志输出量大导致磁盘写满: 减少日志输出 增大磁盘空间 减小单机可调度的pod数量 可写层量大导致磁盘写满: 优化程序逻辑，不写文件到容器内或控制写入文件的大小与数量 镜像占用空间大导致磁盘写满: 增大磁盘空间 删除不需要的镜像 附录 查看docker的磁盘空间占用情况 $ docker system df -v 定位容器写满磁盘的原因 进入容器数据目录(假设是 /var/lib/docker，并且存储驱动是 aufs): $ cd /var/lib/docker $ du -sh * containers 目录: 体积大说明日志输出量大 aufs 目录 diff 子目录: 容器可写层，体积大说明可写层数据量大(程序在容器里写入文件) mnt 子目录: 联合挂载点，内容为容器里看到的内容，即包含镜像本身内容以及可写层内容 找出日志输出量大的 pod TKE 的 pod 中每个容器输出的日志最大存储 1G (日志轮转，最大10个文件，每个文件最大100m，可用 docker inpect 查看): $ docker inspect fef835ebfc88 [ { ... \"HostConfig\": { ... \"LogConfig\": { \"Type\": \"json-file\", \"Config\": { \"max-file\": \"10\", \"max-size\": \"100m\" } }, ... 查看哪些容器日志输出量大： $ cd /var/lib/docker/containers $ du -sh * 目录名即为容器id，使用前几位与 docker ps 结果匹配可找出对应容器，最后就可以推算出是哪些 pod 搞的鬼 找出可写层数据量大的 pod 可写层的数据主要是容器内程序自身写入的，无法控制大小，可写层越大说明容器写入的文件越多或越大，通常是容器内程序将log写到文件里了，查看一下哪个容器的可写层数据量大： $ cd /var/lib/docker/aufs/diff $ du -sh * 通过可写层目录(diff的子目录)反查容器id: $ grep 834d97500892f56b24c6e63ffd4e520fc29c6c0d809a3472055116f59fb1d2be /var/lib/docker/image/aufs/layerdb/mounts/*/mount-id /var/lib/docker/image/aufs/layerdb/mounts/eb76fcd31dfbe5fc949b67e4ad717e002847d15334791715ff7d96bb2c8785f9/mount-id:834d97500892f56b24c6e63ffd4e520fc29c6c0d809a3472055116f59fb1d2be mounts 后面一级的id即为容器id: eb76fcd31dfbe5fc949b67e4ad717e002847d15334791715ff7d96bb2c8785f9，使用前几位与 docker ps 结果匹配可找出对应容器，最后就可以推算出是哪些 pod 搞的鬼 找出体积大的镜像 看看哪些镜像比较占空间 © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"best-practice/efficient-kubectl.html":{"url":"best-practice/efficient-kubectl.html","title":"kubectl 高效技巧","keywords":"","body":"kubectl 高效技巧 是否有过因为使用 kubectl 经常需要重复输入命名空间而苦恼？是否觉得应该要有个记住命名空间的功能，自动记住上次使用的命名空间，不需要每次都输入？可惜没有这种功能，但是，本文会教你一个非常巧妙的方法完美帮你解决这个痛点。 k 命令 将如下脚本粘贴到当前shell(注册k命令到当前终端session): function k() { cmdline=`HISTTIMEFORMAT=\"\" history | awk '$2 == \"kubectl\" && (/-n/ || /--namespace/) {for(i=2;i mac 用户可以使用 dash 的 snippets 功能快速将上面的函数粘贴，使用 kk. 作为触发键 (dash snippets可以全局监听键盘输入，使用指定的输入作为触发而展开配置的内容，相当于是全局代码片段)，以后在某个终端想使用 k 的时候按下 kk. 就可以将 k 命令注册到当前终端，dash snippets 配置如图所示： 将 k 当作 kubectl 来用，只是不需要输入命名空间，它会调用 kubectl 并自动加上上次使用的非默认的命名空间，如果想切换命名空间，再常规的使用一次 kubectl 就行，下面是示范： 哈哈，是否感觉可以少输入很多字符，提高 kubectl 使用效率了？这是目前我探索解决 kubectl 重复输入命名空间的最好方案，一开始是受 fuck命令 的启发，想用 go 语言开发个 k 命令，但是发现两个缺点： 需要安装二进制才可以使用（对于需要在多个地方用kubectl管理多个集群的人来说实在太麻烦） 如果当前 shell 默认没有将历史输入记录到 history 文件( bash 的 history 文件默认是 ~/.bash_history)，那么将无法准确知道上一次 kubectl 使用的哪个命名空间 这里解释下第二个缺点的原因：ssh 连上服务器会启动一个 shell 进程，通常是 bash，大多 bash 默认配置会实时将历史输入追加到 ~/.bash_history里，所以开多个ssh使用history命令看到的历史输入是一样的，但有些默认不会实时记录历史到~/.bash_history，而是记在当前 shell 进程的内存中，在 shell 退出时才会写入到文件。这种情况新起的进程是无法知道当前 shell 的最近历史输入的，fuck命令 也不例外。 所以最完美的解决方案就是注册函数到当前shell来调用，配合 dash 的 snippets 功能可以实现快速注册，解决复制粘贴的麻烦 © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"best-practice/wildcard-domain-forward.html":{"url":"best-practice/wildcard-domain-forward.html","title":"泛域名动态 Service 转发解决方案","keywords":"","body":"泛域名动态 Service 转发解决方案 需求 集群对外暴露了一个公网IP作为流量入口(可以是 Ingress 或 Service)，DNS 解析配置了一个泛域名指向该IP（比如 *.test.imroc.io），现希望根据请求中不同 Host 转发到不同的后端 Service。比如 a.test.imroc.io 的请求被转发到 my-svc-a，b.test.imroc.io 的请求转发到 my-svc-b 简单做法 先说一种简单的方法，这也是大多数人的第一反应：配置 Ingress 规则 假如泛域名有两个不同 Host 分别转发到不同 Service，Ingress 类似这样写: apiVersion: extensions/v1beta1 kind: Ingress metadata: name: my-ingress spec: rules: - host: a.test.imroc.io http: paths: - backend: serviceName: my-svc-a servicePort: 80 path: / - host: b.test.imroc.io http: paths: - backend: serviceName: my-svc-b servicePort: 80 path: / 但是！如果 Host 非常多会怎样？（比如200+） 每次新增 Host 都要改 Ingress 规则，太麻烦 单个 Ingress 上面的规则越来越多，更改规则对 LB 的压力变大，可能会导致偶尔访问不了 正确姿势 我们可以约定请求中泛域名 Host 通配符的 * 号匹配到的字符跟 Service 的名字相关联（可以是相等，或者 Service 统一在前面加个前缀，比如 a.test.imroc.io 转发到 my-svc-a 这个 Service)，集群内起一个反向代理服务，匹配泛域名的请求全部转发到这个代理服务上，这个代理服务只做一件简单的事，解析 Host，正则匹配抓取泛域名中 * 号这部分，把它转换为 Service 名字，然后在集群里转发（集群 DNS 解析) 这个反向代理服务可以是 Nginx+Lua脚本 来实现，或者自己写个简单程序来做反向代理，这里我用 OpenResty 来实现，它可以看成是 Nginx 的发行版，自带 lua 支持。 有几点需要说明下： 我们使用 nginx 的 proxy_pass 来反向代理到后端服务，proxy_pass 后面跟的变量，我们需要用 lua 来判断 Host 修改变量 nginx 的 proxy_pass 后面跟的如果是可变的域名（非IP，需要 dns 解析)，它需要一个域名解析器，不会走默认的 dns 解析，需要在 nginx.conf 里添加 resolver 配置项来设置一个外部的 dns 解析器 这个解析器我们是用 go-dnsmasq 来实现，它可以将集群的 dns 解析代理给 nginx，以 sidecar 的形式注入到 pod 中，监听 53 端口 nginx.conf 里关键的配置如下图所示： 下面给出完整的 yaml 示例 proxy.yaml: apiVersion: apps/v1beta1 kind: Deployment metadata: labels: component: nginx name: proxy spec: replicas: 1 selector: matchLabels: component: nginx template: metadata: labels: component: nginx spec: containers: - name: nginx image: \"openresty/openresty:centos\" ports: - name: http containerPort: 80 protocol: TCP volumeMounts: - mountPath: /usr/local/openresty/nginx/conf/nginx.conf name: config subPath: nginx.conf - name: dnsmasq image: \"janeczku/go-dnsmasq:release-1.0.7\" args: - --listen - \"127.0.0.1:53\" - --default-resolver - --append-search-domains - --hostsfile=/etc/hosts - --verbose volumes: - name: config configMap: name: configmap-nginx --- apiVersion: v1 kind: ConfigMap metadata: labels: component: nginx name: configmap-nginx data: nginx.conf: |- worker_processes 1; error_log /error.log; events { accept_mutex on; multi_accept on; use epoll; worker_connections 1024; } http { include mime.types; default_type application/octet-stream; log_format main '$time_local $remote_user $remote_addr $host $request_uri $request_method $http_cookie ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\" ' '$request_time $upstream_response_time \"$upstream_cache_status\"'; log_format browser '$time_iso8601 $cookie_km_uid $remote_addr $host $request_uri $request_method ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\" ' '$request_time $upstream_response_time \"$upstream_cache_status\" $http_x_requested_with $http_x_real_ip $upstream_addr $request_body'; log_format client '{\"@timestamp\":\"$time_iso8601\",' '\"time_local\":\"$time_local\",' '\"remote_user\":\"$remote_user\",' '\"http_x_forwarded_for\":\"$http_x_forwarded_for\",' '\"host\":\"$server_addr\",' '\"remote_addr\":\"$remote_addr\",' '\"http_x_real_ip\":\"$http_x_real_ip\",' '\"body_bytes_sent\":$body_bytes_sent,' '\"request_time\":$request_time,' '\"status\":$status,' '\"upstream_response_time\":\"$upstream_response_time\",' '\"upstream_response_status\":\"$upstream_status\",' '\"request\":\"$request\",' '\"http_referer\":\"$http_referer\",' '\"http_user_agent\":\"$http_user_agent\"}'; access_log /access.log main; sendfile on; keepalive_timeout 120s 100s; keepalive_requests 500; send_timeout 60000s; client_header_buffer_size 4k; proxy_ignore_client_abort on; proxy_buffers 16 32k; proxy_buffer_size 64k; proxy_busy_buffers_size 64k; proxy_send_timeout 60000; proxy_read_timeout 60000; proxy_connect_timeout 60000; proxy_cache_valid 200 304 2h; proxy_cache_valid 500 404 2s; proxy_cache_key $host$request_uri$cookie_user; proxy_cache_methods GET HEAD POST; proxy_redirect off; proxy_http_version 1.1; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Frame-Options SAMEORIGIN; server_tokens off; client_max_body_size 50G; add_header X-Cache $upstream_cache_status; autoindex off; resolver 127.0.0.1:53 ipv6=off; server { listen 80; location / { set $service ''; rewrite_by_lua ' local host = ngx.var.host local m = ngx.re.match(host, \"(.+).test.imroc.io\") if m then ngx.var.service = \"my-svc-\" .. m[1] end '; proxy_pass http://$service; } } } 让该代理服务暴露公网访问可以用 Service 或 Ingress 用 Service 的示例 (service.yaml): apiVersion: v1 kind: Service metadata: labels: component: nginx name: service-nginx spec: type: LoadBalancer ports: - name: http port: 80 targetPort: http selector: component: nginx 用 Ingress 的示例 (ingress.yaml): apiVersion: extensions/v1beta1 kind: Ingress metadata: name: ingress-nginx spec: rules: - host: \"*.test.imroc.io\" http: paths: - backend: serviceName: service-nginx servicePort: 80 path: / © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"best-practice/handle-memory-fragmentation.html":{"url":"best-practice/handle-memory-fragmentation.html","title":"处理内存碎片化","keywords":"","body":"处理内存碎片化 内存碎片化造成的危害 节点的内存碎片化严重，导致docker运行容器时，无法分到大的内存块，导致start docker失败。最终导致服务更新时，状态一直都是启动中 判断是否内存碎片化严重 内核日志显示： 进一步查看的系统内存(cache多可能是io导致的，为了提高io效率留下的缓存，这部分内存实际是可以释放的)： 查看slab (后面的0多表示伙伴系统没有大块内存了)： 解决方法 周期性地或者在发现大块内存不足时，先进行drop_cache操作:echo 3 > /proc/sys/vm/drop_caches 必要时候进行内存整理，开销会比较大，会造成业务卡住一段时间(慎用):echo 1 > /proc/sys/vm/compact_memory 附录 相关链接： https://www.lijiaocn.com/%E9%97%AE%E9%A2%98/2017/11/13/problem-unable-create-nf-conn.html https://blog.csdn.net/wqhlmark64/article/details/79143975 https://huataihuang.gitbooks.io/cloud-atlas/content/os/linux/kernel/memory/drop_caches_and_compact_memory.html © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "},"best-practice/scale-keepalive-service.html":{"url":"best-practice/scale-keepalive-service.html","title":"解决长连接服务扩容失效","keywords":"","body":"解决长连接服务扩容失效 在现网运营中，有很多场景为了提高效率，一般都采用建立长连接的方式来请求。我们发现在客户端以长连接请求服务端的场景下，K8S的自动扩容会失效。原因是客户端长连接一直保留在老的Pod容器中，新扩容的Pod没有新的连接过来，导致K8S按照步长扩容第一批Pod之后就停止了扩容操作，而且新扩容的Pod没能承载请求，进而出现服务过载的情况，自动扩容失去了意义。 对长连接扩容失效的问题，我们的解决方法是将长连接转换为短连接。我们参考了 nginx keepalive 的设计，nginx 中 keepalive_requests 这个配置项设定了一个TCP连接能处理的最大请求数，达到设定值(比如1000)之后服务端会在 http 的 Header 头标记 “Connection:close”，通知客户端处理完当前的请求后关闭连接，新的请求需要重新建立TCP连接，所以这个过程中不会出现请求失败，同时又达到了将长连接按需转换为短连接的目的。通过这个办法客户端和云K8S服务端处理完一批请求后不断的更新TCP连接，自动扩容的新Pod能接收到新的连接请求，从而解决了自动扩容失效的问题。 由于Golang并没有提供方法可以获取到每个连接处理过的请求数，我们重写了 net.Listener 和 net.Conn，注入请求计数器，对每个连接处理的请求做计数，并通过 net.Conn.LocalAddr() 获得计数值，判断达到阈值 1000 后在返回的 Header 中插入 “Connection:close” 通知客户端关闭连接，重新建立连接来发起请求。以上处理逻辑用 Golang 实现示例代码如下： package main import ( \"net\" \"github.com/gin-gonic/gin\" \"net/http\" ) //重新定义net.Listener type counterListener struct { net.Listener } //重写net.Listener.Accept(),对接收到的连接注入请求计数器 func (c *counterListener) Accept() (net.Conn, error) { conn, err := c.Listener.Accept() if err != nil { return nil, err } return &counterConn{Conn: conn}, nil } //定义计数器counter和计数方法Increment() type counter int func (c *counter) Increment() int { *c++ return int(*c) } //重新定义net.Conn,注入计数器ct type counterConn struct { net.Conn ct counter } //重写net.Conn.LocalAddr()，返回本地网络地址的同时返回该连接累计处理过的请求数 func (c *counterConn) LocalAddr() net.Addr { return &counterAddr{c.Conn.LocalAddr(), &c.ct} } //定义TCP连接计数器,指向连接累计请求的计数器 type counterAddr struct { net.Addr *counter } func main() { r := gin.New() r.Use(func(c *gin.Context) { localAddr := c.Request.Context().Value(http.LocalAddrContextKey) if ct, ok := localAddr.(interface{ Increment() int }); ok { if ct.Increment() >= 1000 { c.Header(\"Connection\", \"close\") } } c.Next() }) r.GET(\"/\", func(c *gin.Context) { c.String(200, \"plain/text\", \"hello\") }) l, err := net.Listen(\"tcp\", \":8080\") if err != nil { panic(err) } err = http.Serve(&counterListener{l}, r) if err != nil { panic(err) } } © roc all right reserved，powered by GitbookUpdated at 2019-12-14 03:07:53 "}}